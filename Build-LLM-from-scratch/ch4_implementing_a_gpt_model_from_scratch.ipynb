{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678a9f74",
   "metadata": {},
   "source": [
    "## 4.1 编码一个LLM架构\n",
    "本章我们将会编写GPT-2模型架构。之所以选择该架构，是因为GPT-2的参数规模可以直接在用户级GPU上训练，并且OpenAI已经公开了其参数。GPT-3与GPT-2架构类似，只是参数规模从1.5B提升到了175B，因此无法在本地训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db39dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020d9ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "        self.trf_blocks = nn.Sequential(*[\n",
    "            DummyTransformerBlock(cfg) for _ in range(cfg['n_layers'])\n",
    "        ])\n",
    "        self.final_norm = DummyLayerNorm(cfg)\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "    def forward(self, in_idx):\n",
    "        # in_idx.shape = [batch_size, seq_len]\n",
    "        bs, seq_len = in_idx.shape\n",
    "        x = self.tok_emb(in_idx) # [batch_size, seq_len, emb_dim]\n",
    "        pos_embed = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        ) # [seq_len, emb_dim]\n",
    "        x = x + pos_embed\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        return\n",
    "    \n",
    "    def forward(self):\n",
    "        return\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        return\n",
    "    def forward(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e1cfa4",
   "metadata": {},
   "source": [
    "## 4.2 实现归一化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0aa2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(embed_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len, emb_dim = x.shape\n",
    "        # 期望和方差的计算都是逐样本的\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        # 注意：此处使用有偏方差是为了和GPT-2源码对齐，方便后续导入参数\n",
    "        var = torch.var(x, dim=-1, keepdim=True, unbiased=False)\n",
    "        x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        # 缩放和偏移是逐特征的，对于不同特征可能需要不同的尺度\n",
    "        x = self.scale * x + self.shift\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ef7f4",
   "metadata": {},
   "source": [
    "* 为什么不省略归一化，直接学 γ 和 β？\t因为归一化提供了训练稳定性，而直接学习仿射变换容易受输入分布影响\n",
    "* 两者数学上等价吗？\t不等价，LayerNorm 是非线性操作，且解耦了优化过程\n",
    "* γ 和 β 是多余的吗？\t❌ 不是！它们让网络可以有选择地恢复所需的分布\n",
    "* LayerNorm 的核心价值是什么？\t解耦 + 稳定 + 加速收敛，而不是单纯的“变换”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3e1e1",
   "metadata": {},
   "source": [
    "Layer Norm VS. Batch Norm\n",
    "如果您熟悉批量归一化（神经网络的一种常见且传统的归一化方法），您可能想知道它与层归一化相比如何。与批量归一化不同，批量归一化在整个批次维度上归一化，层归一化在特征维度上归一化。\n",
    "\n",
    "可用的硬件或特定用例可以决定训练或推理期间的批量大小。由于层归一化独立于批量大小规范化每个输入，因此它在这些场景中提供了更大的灵活性和稳定性。这对于分布式训练或在资源受限的环境中部署模型时特别有利。\n",
    "\n",
    "下面先编写GPT-2中用到的激活函数GELU，然后用其实现前馈层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc3287e",
   "metadata": {},
   "source": [
    "## 4.3 用GELU激活函数实现前馈层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbeeca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * ( 1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], 4*cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg['emb_dim'], cfg['emb_dim']),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87307abf",
   "metadata": {},
   "source": [
    "GELU 的平滑度可以在训练过程中带来更好的优化属性，因为它允许对模型参数进行更细致的调整。相比之下，ReLU 在零处有一个尖角，这有时会使优化变得更加困难，尤其是在非常深或具有复杂架构的网络中。此外，与对任何负输入输出零的 ReLU 不同，GELU 允许对负值输出较小的非零输出。这一特征意味着在训练过程中，接受负输入的神经元仍然可以为学习过程做出贡献，尽管程度低于正输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cbb3f4",
   "metadata": {},
   "source": [
    "## 4.4 添加残差连接\n",
    "\n",
    "残差连接是为了解决深度神经网络中梯度消失的问题。当网络层数过多时，由于反向传播的链式法则，深层梯度会由于小于1的系数累乘而变得很小，使得权重更新缓慢。为了解决，我们可以直接将输入连接至输出，为深层网络添加直接更新权重的路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e90537f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用一个例子演示\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n",
    "            ]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer(x)\n",
    "            # 必须保证输入输出的形状相同才能连接\n",
    "            if self.use_shortcut and x.shape == layer_out.shape:\n",
    "                x = x + layer_out\n",
    "            else:\n",
    "                x = layer_out\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e7e09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[-1., 0., 1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shorcut = ExampleDeepNeuralNetwork(layer_sizes=layer_size, use_shortcut=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccfdc0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    target = torch.tensor([[0.]])\n",
    "    output = model(x)\n",
    "    metric = nn.MSELoss()\n",
    "    loss = metric(target, output)\n",
    "    loss.backward()\n",
    "    for name, p in model.named_parameters():\n",
    "        # 只打印权重，不要bias\n",
    "        if 'weight' in name:\n",
    "            print(f'{name} has gradient mean of {p.grad.abs().mean().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "556ae7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.0002546171599533409\n",
      "layers.1.0.weight has gradient mean of 8.308066026074812e-05\n",
      "layers.2.0.weight has gradient mean of 0.0007468174444511533\n",
      "layers.3.0.weight has gradient mean of 0.001237659016624093\n",
      "layers.4.0.weight has gradient mean of 0.004640596453100443\n"
     ]
    }
   ],
   "source": [
    "# 观察到梯度的消失\n",
    "print_gradients(model_without_shorcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5a2c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[-1., 0., 1.]])\n",
    "torch.manual_seed(123)\n",
    "model_with_shorcut = ExampleDeepNeuralNetwork(layer_sizes=layer_size, use_shortcut=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e62362c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.022350559011101723\n",
      "layers.1.0.weight has gradient mean of 0.04712466523051262\n",
      "layers.2.0.weight has gradient mean of 0.02791227586567402\n",
      "layers.3.0.weight has gradient mean of 0.013673197478055954\n",
      "layers.4.0.weight has gradient mean of 0.23397451639175415\n"
     ]
    }
   ],
   "source": [
    "# 观察到梯度的稳定。\n",
    "print_gradients(model_with_shorcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca03c4aa",
   "metadata": {},
   "source": [
    "## 4.5 将注意力和线性层组合成transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5170191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n",
      "context vector v1:\n",
      " tensor([[0.9173, 0.4695, 0.9805, 0.6120],\n",
      "        [0.9572, 0.4880, 1.0261, 0.6365],\n",
      "        [0.9564, 0.4877, 1.0250, 0.6361],\n",
      "        [0.9019, 0.4629, 0.9625, 0.6030],\n",
      "        [0.9080, 0.4668, 0.9653, 0.6084],\n",
      "        [0.9150, 0.4685, 0.9790, 0.6104]], grad_fn=<MmBackward0>)\n",
      "context_vector v2:\n",
      "  tensor([[-0.1180, -0.0476, -0.1710,  0.2517],\n",
      "        [-0.1190, -0.0470, -0.1756,  0.2561],\n",
      "        [-0.1192, -0.0469, -0.1758,  0.2563],\n",
      "        [-0.1195, -0.0466, -0.1756,  0.2562],\n",
      "        [-0.1214, -0.0458, -0.1777,  0.2592],\n",
      "        [-0.1184, -0.0472, -0.1744,  0.2546]], grad_fn=<MmBackward0>)\n",
      "context vector v1:\n",
      " tensor([[-0.1180, -0.0476, -0.1710,  0.2517],\n",
      "        [-0.1190, -0.0470, -0.1756,  0.2561],\n",
      "        [-0.1192, -0.0469, -0.1758,  0.2563],\n",
      "        [-0.1195, -0.0466, -0.1756,  0.2562],\n",
      "        [-0.1214, -0.0458, -0.1777,  0.2592],\n",
      "        [-0.1184, -0.0472, -0.1744,  0.2546]], grad_fn=<MmBackward0>)\n",
      "context_vector v2:\n",
      "  tensor([[-0.1180, -0.0476, -0.1710,  0.2517],\n",
      "        [-0.1190, -0.0470, -0.1756,  0.2561],\n",
      "        [-0.1192, -0.0469, -0.1758,  0.2563],\n",
      "        [-0.1195, -0.0466, -0.1756,  0.2562],\n",
      "        [-0.1214, -0.0458, -0.1777,  0.2592],\n",
      "        [-0.1184, -0.0472, -0.1744,  0.2546]], grad_fn=<MmBackward0>)\n",
      "tensor([[0.1709, 0.1641, 0.1644, 0.1658, 0.1721, 0.1627],\n",
      "        [0.1719, 0.1685, 0.1683, 0.1631, 0.1636, 0.1646],\n",
      "        [0.1716, 0.1687, 0.1685, 0.1631, 0.1634, 0.1647],\n",
      "        [0.1700, 0.1676, 0.1675, 0.1647, 0.1643, 0.1658],\n",
      "        [0.1655, 0.1719, 0.1716, 0.1632, 0.1613, 0.1665],\n",
      "        [0.1727, 0.1656, 0.1656, 0.1650, 0.1659, 0.1651]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1709, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1719, 0.1685, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1716, 0.1687, 0.1685, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1700, 0.1676, 0.1675, 0.1647, 0.0000, 0.0000],\n",
      "        [0.1655, 0.1719, 0.1716, 0.1632, 0.1613, 0.0000],\n",
      "        [0.1727, 0.1656, 0.1656, 0.1650, 0.1659, 0.1651]],\n",
      "       grad_fn=<TrilBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5050, 0.4950, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3373, 0.3315, 0.3312, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2538, 0.2502, 0.2501, 0.2459, 0.0000, 0.0000],\n",
      "        [0.1986, 0.2062, 0.2059, 0.1958, 0.1935, 0.0000],\n",
      "        [0.1727, 0.1656, 0.1656, 0.1650, 0.1659, 0.1651]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[-0.0030,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.1569,  0.1166,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.1594,  0.1247,  0.1231,    -inf,    -inf,    -inf],\n",
      "        [ 0.0919,  0.0634,  0.0623,  0.0285,    -inf,    -inf],\n",
      "        [ 0.1598,  0.2356,  0.2324,  0.1319,  0.1086,    -inf],\n",
      "        [ 0.0728, -0.0112, -0.0112, -0.0186, -0.0082, -0.0182]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5050, 0.4950, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3373, 0.3315, 0.3312, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2538, 0.2502, 0.2501, 0.2459, 0.0000, 0.0000],\n",
      "        [0.1986, 0.2062, 0.2059, 0.1958, 0.1935, 0.0000],\n",
      "        [0.1727, 0.1656, 0.1656, 0.1650, 0.1659, 0.1651]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[2., 2., 2., 2., 0., 2.],\n",
      "        [0., 0., 2., 2., 2., 2.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [0., 2., 2., 0., 2., 2.],\n",
      "        [2., 2., 0., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 0., 0.]])\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6625, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5004, 0.5002, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3971, 0.4125, 0.0000, 0.3916, 0.3871, 0.0000],\n",
      "        [0.3455, 0.3313, 0.3313, 0.3301, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([6, 3])\n",
      "torch.Size([2, 6, 3])\n",
      "torch.Size([2, 6, 2])\n",
      "tensor([[[ 0.4241, -0.0799],\n",
      "         [ 0.3581, -0.2314],\n",
      "         [ 0.3367, -0.2850],\n",
      "         [ 0.2791, -0.2640],\n",
      "         [ 0.2754, -0.2999],\n",
      "         [ 0.2480, -0.2722]],\n",
      "\n",
      "        [[ 0.4241, -0.0799],\n",
      "         [ 0.3581, -0.2314],\n",
      "         [ 0.3367, -0.2850],\n",
      "         [ 0.2791, -0.2640],\n",
      "         [ 0.2754, -0.2999],\n",
      "         [ 0.2480, -0.2722]]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "from ch3 import MultiHeadAttention\n",
    "\n",
    "# GPT_CONFIG_124M = {\n",
    "#     \"vocab_size\": 50257,\n",
    "#     \"context_length\": 1024,\n",
    "#     \"emb_dim\": 768,\n",
    "#     \"n_heads\": 12,\n",
    "#     \"n_layers\": 12,\n",
    "#     \"drop_rate\": 0.1,\n",
    "#     \"qkv_bias\": False\n",
    "# }\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_in=cfg['emb_dim'],\n",
    "            d_out=cfg['emb_dim'],\n",
    "            num_heads=cfg['n_heads'],\n",
    "            context_len=cfg['context_length'],\n",
    "            dropout=cfg['drop_rate'],\n",
    "            qkv_bias=cfg['qkv_bias'],\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.dropout_shorcut = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len, emb_dim = x.shape\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout_shorcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout_shorcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc02b78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([2, 4, 768])\n",
      "Output Shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "x = torch.rand(2, 4, 768)\n",
    "torch.manual_seed(42)\n",
    "trf_block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = trf_block(x)\n",
    "\n",
    "print('Input Shape:', x.shape)\n",
    "print('Output Shape:', output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ed2de",
   "metadata": {},
   "source": [
    "## 4.6 编写GPT模型\n",
    "\n",
    "我们已经实现了TransformerBlock和LayerNorm，只需要将上面写的DummyGPTModel中的模块替换成写好的模块就完成了GPT模型的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8da0fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "        self.trf_blocks = nn.Sequential(*[\n",
    "            TransformerBlock(cfg) for _ in range(cfg['n_layers'])\n",
    "        ])\n",
    "        self.final_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "    def forward(self, in_idx):\n",
    "        # in_idx.shape = [batch_size, seq_len]\n",
    "        bs, seq_len = in_idx.shape\n",
    "        x = self.tok_emb(in_idx) # [batch_size, seq_len, emb_dim]\n",
    "        pos_embed = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        ) # [seq_len, emb_dim]\n",
    "        x = x + pos_embed\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfa2b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4]) torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "input = torch.randint(0, 50256, (2, 4))\n",
    "output = model(input)\n",
    "print(input.shape, output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67891cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {total_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d2fb0",
   "metadata": {},
   "source": [
    "之所以这里参数是163M而不是我们期待的124M，是因为原始的GPT-2采用了weight tying，即tok_emb和out_head是共用的，这会减少很多参数，但是会降低模型性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d3b7198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 # float32 = 4 bytes\n",
    "total_size_mb = total_size_bytes / ( 1024 * 1024)\n",
    "print(f'Total size of the model {total_size_mb:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22221b",
   "metadata": {},
   "source": [
    "## 4.7 生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f826f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写一个函数用迭代的方法逐步生成新文本\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_len):\n",
    "    # idx.shape=[bs, seq_len]\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            output = model(idx[-context_len:]) # [bs, seq_len, vocab_size]\n",
    "        output_token = output[:, -1, :]\n",
    "        prob = torch.softmax(output_token, dim=-1)\n",
    "        new_idx = torch.argmax(prob, dim=-1, keepdim=True)\n",
    "        idx = torch.cat([idx, new_idx], dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daaee032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input:[15496, 11, 314, 716]\n",
      "Encoded tensor shape:torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "started_context = 'Hello, I am'\n",
    "encoded = tokenizer.encode(started_context)\n",
    "print(f'Encoded input:{encoded}')\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(f'Encoded tensor shape:{encoded_tensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d8f63f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor:tensor([[15496,    11,   314,   716,  4754, 22091, 43072, 19101, 14187, 41501]])\n",
      "Output shape:torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "output = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_len=GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "print(f'Output tensor:{output}')\n",
    "print(f'Output shape:{output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66de464f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded output:Hello, I amulf Kai cog Portugal paStudio\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(output.squeeze(0).tolist())\n",
    "print(f'Decoded output: {decoded}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wqllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
