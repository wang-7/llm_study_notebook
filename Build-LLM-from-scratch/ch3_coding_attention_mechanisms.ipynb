{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc76e0d2",
   "metadata": {},
   "source": [
    "## 3.4 用一个类实现自注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bcbb926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [seq_len, embed_dim]\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        # attn score: [seq_len, seq_len], 表示每个token和每个token的相关性分数\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db6d52f",
   "metadata": {},
   "source": [
    "注意到上面的qkv参数矩阵是用nn.Parameter手动赋值的，我们也可以用nn.Linear来实现更好的初始化策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042cf66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [seq_len, embed_dim]\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # attn score: [seq_len, seq_len], 表示每个token和每个token的相关性分数\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2376d459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n",
      "context vector v1:\n",
      " tensor([[0.9173, 0.4695, 0.9805, 0.6120],\n",
      "        [0.9572, 0.4880, 1.0261, 0.6365],\n",
      "        [0.9564, 0.4877, 1.0250, 0.6361],\n",
      "        [0.9019, 0.4629, 0.9625, 0.6030],\n",
      "        [0.9080, 0.4668, 0.9653, 0.6084],\n",
      "        [0.9150, 0.4685, 0.9790, 0.6104]], grad_fn=<MmBackward0>)\n",
      "context_vector v2:\n",
      "  tensor([[-0.1180, -0.0476, -0.1710,  0.2517],\n",
      "        [-0.1190, -0.0470, -0.1756,  0.2561],\n",
      "        [-0.1192, -0.0469, -0.1758,  0.2563],\n",
      "        [-0.1195, -0.0466, -0.1756,  0.2562],\n",
      "        [-0.1214, -0.0458, -0.1777,  0.2592],\n",
      "        [-0.1184, -0.0472, -0.1744,  0.2546]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 下面构造一个输入数据来进行测试\n",
    "seq_len = 6\n",
    "embed_dim = 3\n",
    "torch.manual_seed(42)\n",
    "inputs = torch.tensor( [[0.43, 0.15, 0.89], # Your (x^1) \n",
    "                        [0.55, 0.87, 0.66], # journey (x^2) \n",
    "                        [0.57, 0.85, 0.64], # starts (x^3) \n",
    "                        [0.22, 0.58, 0.33], # with (x^4) \n",
    "                        [0.77, 0.25, 0.10], # one (x^5) \n",
    "                        [0.05, 0.80, 0.55]] # step (x^6) \n",
    "                        )\n",
    "print(inputs)\n",
    "\n",
    "d_out = 4\n",
    "sa_v1 = SelfAttention_v1(embed_dim, d_out)\n",
    "sa_v2 = SelfAttention_v2(embed_dim, d_out)\n",
    "\n",
    "context_vec1 = sa_v1(inputs)\n",
    "context_vec2= sa_v2(inputs)\n",
    "print('context vector v1:\\n', context_vec1)\n",
    "print('context_vector v2:\\n ', context_vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a94ac5f",
   "metadata": {},
   "source": [
    "可以观察到，v1和v2因为参数矩阵的初始化不同，导致输出不同，我们可以将v2的参数矩阵复制到v1上去，使其输出相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d124a52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context vector v1:\n",
      " tensor([[-0.1180, -0.0476, -0.1710,  0.2517],\n",
      "        [-0.1190, -0.0470, -0.1756,  0.2561],\n",
      "        [-0.1192, -0.0469, -0.1758,  0.2563],\n",
      "        [-0.1195, -0.0466, -0.1756,  0.2562],\n",
      "        [-0.1214, -0.0458, -0.1777,  0.2592],\n",
      "        [-0.1184, -0.0472, -0.1744,  0.2546]], grad_fn=<MmBackward0>)\n",
      "context_vector v2:\n",
      "  tensor([[-0.1180, -0.0476, -0.1710,  0.2517],\n",
      "        [-0.1190, -0.0470, -0.1756,  0.2561],\n",
      "        [-0.1192, -0.0469, -0.1758,  0.2563],\n",
      "        [-0.1195, -0.0466, -0.1756,  0.2562],\n",
      "        [-0.1214, -0.0458, -0.1777,  0.2592],\n",
      "        [-0.1184, -0.0472, -0.1744,  0.2546]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# print(sa_v1.state_dict())\n",
    "# print(sa_v2.state_dict())\n",
    "#\n",
    "sa_v1.W_key.data.copy_(sa_v2.W_key.weight.data.T)\n",
    "sa_v1.W_query.data.copy_(sa_v2.W_query.weight.data.T)\n",
    "sa_v1.W_value.data.copy_(sa_v2.W_value.weight.data.T)\n",
    "\n",
    "context_vec1 = sa_v1(inputs)\n",
    "context_vec2= sa_v2(inputs)\n",
    "print('context vector v1:\\n', context_vec1)\n",
    "print('context_vector v2:\\n ', context_vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ebaa92",
   "metadata": {},
   "source": [
    "## 3.5 用因果注意力隐藏未来的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4106c7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1709, 0.1641, 0.1644, 0.1658, 0.1721, 0.1627],\n",
      "        [0.1719, 0.1685, 0.1683, 0.1631, 0.1636, 0.1646],\n",
      "        [0.1716, 0.1687, 0.1685, 0.1631, 0.1634, 0.1647],\n",
      "        [0.1700, 0.1676, 0.1675, 0.1647, 0.1643, 0.1658],\n",
      "        [0.1655, 0.1719, 0.1716, 0.1632, 0.1613, 0.1665],\n",
      "        [0.1727, 0.1656, 0.1656, 0.1650, 0.1659, 0.1651]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 首先借用上面写的注意力参数矩阵计算注意力权重\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_score = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_score / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02016a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1709, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1719, 0.1685, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1716, 0.1687, 0.1685, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1700, 0.1676, 0.1675, 0.1647, 0.0000, 0.0000],\n",
      "        [0.1655, 0.1719, 0.1716, 0.1632, 0.1613, 0.0000],\n",
      "        [0.1727, 0.1656, 0.1656, 0.1650, 0.1659, 0.1651]],\n",
      "       grad_fn=<TrilBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = keys.shape[0]\n",
    "# 使用tril函数生成一个矩阵的下三角矩阵（其余元素置零）\n",
    "mask_simple = torch.tril(attn_weights)\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26cddf",
   "metadata": {},
   "source": [
    "注意到应用tril后，剩下元素不再满足和为1，因此需要重新归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d5e63e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5050, 0.4950, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3373, 0.3315, 0.3312, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2538, 0.2502, 0.2501, 0.2459, 0.0000, 0.0000],\n",
      "        [0.1986, 0.2062, 0.2059, 0.1958, 0.1935, 0.0000],\n",
      "        [0.1727, 0.1656, 0.1656, 0.1650, 0.1659, 0.1651]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_renorm = mask_simple / torch.sum(mask_simple, dim=-1, keepdim=True)\n",
    "print(attn_weights_renorm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f3210",
   "metadata": {},
   "source": [
    "此处需要注意，实际上进行了两次归一化操作，那能否简化呢？观察下面的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e96c4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0030,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.1569,  0.1166,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.1594,  0.1247,  0.1231,    -inf,    -inf,    -inf],\n",
      "        [ 0.0919,  0.0634,  0.0623,  0.0285,    -inf,    -inf],\n",
      "        [ 0.1598,  0.2356,  0.2324,  0.1319,  0.1086,    -inf],\n",
      "        [ 0.0728, -0.0112, -0.0112, -0.0186, -0.0082, -0.0182]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones_like(attn_score), diagonal=1)\n",
    "masked = attn_score.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6253cfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5050, 0.4950, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3373, 0.3315, 0.3312, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2538, 0.2502, 0.2501, 0.2459, 0.0000, 0.0000],\n",
      "        [0.1986, 0.2062, 0.2059, 0.1958, 0.1935, 0.0000],\n",
      "        [0.1727, 0.1656, 0.1656, 0.1650, 0.1659, 0.1651]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d631163",
   "metadata": {},
   "source": [
    "发现这里一步归一化算得的注意力权重和上面的两步归一化算得的权重是一模一样的。这是因为上面两步归一化在数学上等价于在未被掩码的子集上进行归一化，这一结论可以简单地通过定义证明。\n",
    "\n",
    "下面还要实现dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c923374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 0., 2.],\n",
      "        [0., 0., 2., 2., 2., 2.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [0., 2., 2., 0., 2., 2.],\n",
      "        [2., 2., 0., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 构造一个全1的测试矩阵\n",
    "torch.manual_seed(42)\n",
    "example = torch.ones(6, 6)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46600ed",
   "metadata": {},
   "source": [
    "注意到dropout的作用是根据概率随机地将一部分参数置零，同时为了保持数据规模，其余元素除以(1-p)，相当于保持期望不变。通常在self-attention中应用dropout有两个地方，attention weights或者values，我们采用前者。\n",
    "\n",
    "Dropout在训练阶段每次前向传播时都独立地起作用，测试阶段不起作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8cf61b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6625, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5004, 0.5002, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3971, 0.4125, 0.0000, 0.3916, 0.3871, 0.0000],\n",
      "        [0.3455, 0.3313, 0.3313, 0.3301, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 现在可以将dropout应用于注意力权重上了。\n",
    "torch.manual_seed(42)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fe140",
   "metadata": {},
   "source": [
    "最后，我们将上面两种操作（mask，dropout）添加到attention类中，同时让其能够处理多批次输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00b372d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', \n",
    "                             torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x.shape=[batch_size, context_length, d_in]\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # 注意此处有batch维度的存在，不能直接转置（.T运算符会反转所有维度）\n",
    "        attn_scores = queries @ torch.transpose(keys, 1, 2)\n",
    "        # 先对score应用softmax得到权重\n",
    "        attn_scores.masked_fill_(self.mask.bool(), -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1) \n",
    "        # 再对权重应用dropout\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        # 最后计算value\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd248f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "torch.Size([2, 6, 3])\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "batch_inputs = torch.stack([inputs, inputs], dim=0)\n",
    "print(batch_inputs.shape)\n",
    "batch_size, context_len, embed_dim = batch_inputs.shape\n",
    "ca = CausalAttention(embed_dim, 2, context_len, 0.0)\n",
    "context_vec = ca(batch_inputs)\n",
    "print(context_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a8863",
   "metadata": {},
   "source": [
    "## 3.6 将单头注意力扩展为多头注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02e1088",
   "metadata": {},
   "source": [
    "多头注意力的实现，简单来说就是多个单头注意力（上面实现的）并行计算，将结果拼接即可。既然如此，一个简单的方法就是直接用列表包装单头注意力，但这样是用循环的串行计算，非常耗时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c082f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # 使用nn.ModuleList自动注册子模块，否则pytorch不会自动注册列表内的模块。\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "                       for _ in range(num_heads)]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.cat(\n",
    "            [head(x) for head in self.heads], dim=-1\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece411e0",
   "metadata": {},
   "source": [
    "为了将上面的串行计算改为并行，需要用一些矩阵的拼接和分割操作。\n",
    "\n",
    "下面实现了高效的多头注意力机制，也是大模型中实际使用的多头注意力机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ed97238",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        #划分多头\n",
    "        self.num_heads, self.d_out = num_heads, d_out\n",
    "        assert d_out % num_heads == 0, 'd_out must be devisable by num_heads'\n",
    "        self.d_head = d_out // num_heads\n",
    "\n",
    "        # 首先初始化qkv矩阵，注意此处d_out实际上num_heads个长度为d_head的头拼接在一起的\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # 注册mask, dropout层，projection层\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_len, context_len), diagonal=1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(d_out, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step1: 计算qkv，并拆成多头堆叠的形式\n",
    "        # x.shape = [batch_size, context_len, embed_dim]\n",
    "        bs, context_len, embed_dim = x.shape\n",
    "        queries = self.W_q(x).view(bs, context_len, self.num_heads, self.d_head)\n",
    "        keys = self.W_k(x).view(bs, context_len, self.num_heads, self.d_head)\n",
    "        values = self.W_v(x).view(bs, context_len, self.num_heads, self.d_head)\n",
    "        queries, keys, values = queries.transpose(1, 2), keys.transpose(1, 2), values.transpose(1, 2)\n",
    "\n",
    "        # Step2: 计算注意力权重，这一步和单头没有区别，注意softmax中有系数\n",
    "        attn_scores = queries @ keys.transpose(2, 3) # [bs, num_head, context_len, context_len]\n",
    "        attn_scores.masked_fill_(self.mask.bool(), -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Step3: 计算上下文向量，并将多头拼接，注意view之前需要将tensor连续化。最后别忘记projection。\n",
    "        context_vec = attn_weights @ values # [bs, num_head, context_len, d_head]\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous().view(bs, context_len, self.d_out)\n",
    "        context_vec = self.projection(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b504a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4241, -0.0799],\n",
      "         [ 0.3581, -0.2314],\n",
      "         [ 0.3367, -0.2850],\n",
      "         [ 0.2791, -0.2640],\n",
      "         [ 0.2754, -0.2999],\n",
      "         [ 0.2480, -0.2722]],\n",
      "\n",
      "        [[ 0.4241, -0.0799],\n",
      "         [ 0.3581, -0.2314],\n",
      "         [ 0.3367, -0.2850],\n",
      "         [ 0.2791, -0.2640],\n",
      "         [ 0.2754, -0.2999],\n",
      "         [ 0.2480, -0.2722]]], grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "bs, context_len, embed_dim = batch_inputs.shape\n",
    "mha = MultiHeadAttention(embed_dim, 2, 2, 0.0)\n",
    "context_vecs = mha(batch_inputs)\n",
    "print(context_vec)\n",
    "print(context_vec.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wqllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
